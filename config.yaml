# Kolosal Server Configuration
# ============================
#
# This configuration file supports both JSON and YAML formats.
# The server automatically detects the format based on the file extension.
#
# Environment Variable Overrides:
# - KOLOSAL_PORT: Override server.port
# - KOLOSAL_HOST: Override server.host  
# - KOLOSAL_LOG_LEVEL: Override logging.level
# - KOLOSAL_API_KEY: Add to auth.api_keys
# - KOLOSAL_REQUIRE_API_KEY: Override auth.require_api_key
#
# Command line arguments can also override any of these settings.
# For a complete list of options, run: ./kolosal-server --help

# =============================================================================
# Server Configuration
# =============================================================================
server:
  port: "8080"                        # Server port (string format)
  host: "0.0.0.0"                     # Bind host (0.0.0.0 for all interfaces, localhost for local only)

  idle_timeout: 60                   # Connection idle timeout in seconds
  allow_public_access: false          # Set to true to allow access from other devices on your network
  allow_internet_access: false       # Set to true to enable internet access (requires port forwarding)

# =============================================================================
# Logging Configuration  
# =============================================================================
logging:
  level: "INFO"                       # Log level: DEBUG, INFO, WARN, ERROR
  file: "server.log"                  # Log file path (empty = console only)
  access_log: false                   # Enable HTTP access logging for all requests

# =============================================================================
# Authentication & Security Configuration
# =============================================================================

auth:
  enabled: true                       # Enable authentication system
  require_api_key: false              # Require API key for all requests
  
  # API key authentication settings
  api_key_header: "X-API-Key"         # HTTP header name for API keys
  api_keys:                           # List of valid API keys
    # - "your_api_key_here"
    # - "sk-1234567890abcdef"
  
  # Rate limiting configuration
  rate_limit:
    enabled: true                     # Enable rate limiting
    max_requests: 100                 # Maximum requests per window
    window_size: 60                   # Rate limit window size in seconds
    
  # CORS (Cross-Origin Resource Sharing) configuration
  cors:
    enabled: true                     # Enable CORS headers
    allow_credentials: false          # Allow credentials in CORS requests
    max_age: 86400                    # CORS preflight cache duration in seconds (24 hours)
    allowed_origins:                  # Allowed origins (* for all, or specify domains)
      - "*"
    allowed_methods:                  # Allowed HTTP methods
      - "GET"
      - "POST"
      - "PUT"
      - "DELETE"
      - "OPTIONS"
      - "HEAD"
      - "PATCH"
    allowed_headers:                  # Allowed HTTP headers
      - "Content-Type"
      - "Authorization"
      - "X-Requested-With"
      - "Accept"
      - "Origin"

# =============================================================================
# Feature Configuration
# =============================================================================
features:
  health_check: true                  # Enable /health endpoint
  metrics: true                       # Enable /metrics and /completion-metrics endpoints

# =============================================================================
# Model Configuration
# =============================================================================
# Define models to be loaded at startup or made available for lazy loading
models:
  # Example model configuration - Uncomment and modify as needed
  # - id: "qwen3-0.6b"                              # Unique model identifier
  #   path: "https://huggingface.co/kolosal/qwen3-0.6b/resolve/main/Qwen3-0.6B-UD-Q4_K_XL.gguf"
  #   load_immediately: true                         # Load immediately on server start
  #   main_gpu_id: 0                                # Primary GPU ID for inference (-1 for CPU only)
  #   inference_engine: "llama-cpu"                  # Inference engine: llama-cpu, llama-cuda, llama-vulkan, etc.
  #   load_params:
  #     # Context and memory settings
  #     n_ctx: 2048                                 # Context window size (max tokens)
  #     n_keep: 1024                                # Number of tokens to keep in context
  #     
  #     # Memory optimization
  #     use_mmap: true                              # Use memory mapping for model loading
  #     use_mlock: false                            # Lock model in memory (prevents swapping)
  #     
  #     # Processing settings
  #     n_parallel: 1                               # Number of parallel sequences
  #     cont_batching: true                         # Enable continuous batching for better throughput
  #     warmup: false                               # Perform warmup on model load
  #     
  #     # Hardware acceleration
  #     n_gpu_layers: 100                           # Number of layers to offload to GPU
  #     
  #     # Batch processing
  #     n_batch: 2048                               # Batch size for prompt processing
  #     n_ubatch: 512                               # Micro-batch size
  
  # Example local model configuration for production use
  # - id: "production-model"
  #   path: "./models/production-model.gguf"
  #   load_immediately: false                        # Lazy loading for faster startup
  #   main_gpu_id: 0
  #   inference_engine: "llama-cuda"                 # Use CUDA for GPU acceleration
  #   load_params:
  #     n_ctx: 4096                                 # Larger context for complex tasks
  #     n_keep: 2048
  #     use_mmap: true
  #     use_mlock: true                             # Lock in GPU memory for best performance
  #     n_parallel: 4                               # Multiple parallel sequences
  #     cont_batching: true
  #     warmup: true                                # Warmup for optimal performance
  #     n_gpu_layers: 50                            # Adjust based on available GPU memory
  #     n_batch: 1024
  #     n_ubatch: 256