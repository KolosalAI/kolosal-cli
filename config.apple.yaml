# Kolosal Server Configuration for macOS
# =======================================
#
# This configuration file supports both JSON and YAML formats.
# The server automatically detects the format based on the file extension.
#
# Environment Variable Overrides:
# - KOLOSAL_PORT: Override server.port
# - KOLOSAL_HOST: Override server.host  
# - KOLOSAL_LOG_LEVEL: Override logging.level
# - KOLOSAL_API_KEY: Add to auth.api_keys
# - KOLOSAL_REQUIRE_API_KEY: Override auth.require_api_key
#
# Command line arguments can also override any of these settings.
# For a complete list of options, run: ./kolosal-server --help

# =============================================================================
# Server Configuration
# =============================================================================
server:
  port: "8080"                        # Server port (string format)
  host: "127.0.0.1"                   # Bind host (127.0.0.1 for local only, 0.0.0.0 for all interfaces)

  idle_timeout: 60                   # Connection idle timeout in seconds
  allow_public_access: false          # Set to true to allow access from other devices on your network
  allow_internet_access: false       # Set to true to enable internet access (requires port forwarding)

# =============================================================================
# Logging Configuration  
# =============================================================================
logging:
  level: "INFO"                       # Log level: DEBUG, INFO, WARN, ERROR
  file: "~/Library/Logs/Kolosal/server.log"  # Log file path (empty = console only)
  access_log: false                   # Enable HTTP access logging for all requests

# =============================================================================
# Authentication & Security Configuration
# =============================================================================

auth:
  enabled: true                       # Enable authentication system
  require_api_key: false              # Require API key for all requests
  
  # API key authentication settings
  api_key_header: "X-API-Key"         # HTTP header name for API keys
  api_keys:                           # List of valid API keys
    # - "your_api_key_here"
    # - "sk-1234567890abcdef"
  
  # Rate limiting configuration
  rate_limit:
    enabled: true                     # Enable rate limiting
    max_requests: 100                 # Maximum requests per window
    window_size: 60                   # Rate limit window size in seconds
    
  # CORS (Cross-Origin Resource Sharing) configuration
  cors:
    enabled: true                     # Enable CORS headers
    allow_credentials: false          # Allow credentials in CORS requests
    max_age: 86400                    # CORS preflight cache duration in seconds (24 hours)
    allowed_origins:                  # Allowed origins (* for all, or specify domains)
      - "*"
    allowed_methods:                  # Allowed HTTP methods
      - "GET"
      - "POST"
      - "PUT"
      - "DELETE"
      - "OPTIONS"
      - "HEAD"
      - "PATCH"
    allowed_headers:                  # Allowed HTTP headers
      - "Content-Type"
      - "Authorization"
      - "X-Requested-With"
      - "Accept"
      - "Origin"

# =============================================================================
# Feature Configuration
# =============================================================================
features:
  health_check: true                  # Enable /health endpoint
  metrics: true                       # Enable /metrics and /completion-metrics endpoints

# =============================================================================
# Inference Engines Configuration (macOS optimized)
# =============================================================================
# Define inference engines to be made available for model loading
# On macOS, Metal acceleration is preferred over Vulkan/CUDA
inference_engines:
  # Apple Metal inference engine (preferred on macOS)
  - name: "llama-metal"
    library_path: "../Frameworks/libllama-metal.dylib"  # App bundle Frameworks directory
    version: "1.0.0"
    description: "Apple Metal GPU acceleration engine for GGUF models (macOS native)"
    load_on_startup: true               # Load automatically - Metal is the default on macOS

default_inference_engine: "llama-metal"  # Set Metal as the default engine for macOS

# =============================================================================
# Model Configuration (macOS paths)
# =============================================================================
# Define models to be loaded at startup or made available for lazy loading
# Models can be stored in standard macOS locations:
# - ~/Library/Application Support/Kolosal/models/
# - /usr/local/share/kolosal/models/
# - /opt/homebrew/share/kolosal/models/
models:
  # Example model configuration for macOS - Uncomment and modify as needed
  # - id: "qwen3-0.6b"                              # Unique model identifier
  #   path: "https://huggingface.co/kolosal/qwen3-0.6b/resolve/main/Qwen3-0.6B-UD-Q4_K_XL.gguf"
  #   # Or use local path: "~/Library/Application Support/Kolosal/models/qwen3-0.6b.gguf"
  #   load_immediately: true                         # Load immediately on server start
  #   main_gpu_id: 0                                # Primary GPU ID for inference (-1 for CPU only)
  #   inference_engine: "llama-metal"                # Use Metal acceleration by default on macOS
  #   load_params:
  #     # Context and memory settings
  #     n_ctx: 2048                                 # Context window size (max tokens)
  #     n_keep: 1024                                # Number of tokens to keep in context
  #     
  #     # Memory optimization (Apple Silicon unified memory benefits)
  #     use_mmap: true                              # Use memory mapping for model loading
  #     use_mlock: false                            # Generally not needed on macOS due to unified memory
  #     
  #     # Processing settings
  #     n_parallel: 1                               # Number of parallel sequences
  #     cont_batching: true                         # Enable continuous batching for better throughput
  #     warmup: false                               # Perform warmup on model load
  #     
  #     # Metal GPU acceleration (Apple Silicon optimized)
  #     n_gpu_layers: -1                            # -1 = auto-detect optimal layer count for Metal
  #     
  #     # Batch processing (Apple Silicon optimized)
  #     n_batch: 1024                               # Smaller batches often better on Apple Silicon
  #     n_ubatch: 256                               # Micro-batch size

  # Example local model configuration optimized for Apple Silicon
  # - id: "production-model-metal"
  #   path: "~/Library/Application Support/Kolosal/models/production-model.gguf"
  #   load_immediately: false                        # Lazy loading for faster startup
  #   main_gpu_id: 0                                # Use unified memory GPU
  #   inference_engine: "llama-metal"                # Metal acceleration for best performance
  #   load_params:
  #     n_ctx: 4096                                 # Larger context for complex tasks
  #     n_keep: 2048
  #     use_mmap: true                              # Efficient on macOS
  #     use_mlock: false                            # Not needed with unified memory
  #     n_parallel: 2                               # Conservative for Apple Silicon
  #     cont_batching: true
  #     warmup: true                                # Warmup Metal shaders
  #     n_gpu_layers: -1                            # Auto-optimize for Metal
  #     n_batch: 512                                # Optimized for Apple Silicon
  #     n_ubatch: 128
